## Fine-tuning-Qwen2.5
This repository demonstrates how to fineâ€‘tune the Qwen/Qwen2.5â€‘1.5B language model using QLoRA, enabling efficient training on consumer GPUs while achieving strong performance on custom instructionâ€‘based tasks.
The project includes:
- A complete QLoRA fineâ€‘tuning pipeline
- Dataset formatting guidelines
- Evaluation scripts
- Instructions for adapting the pipeline to any domain
This setup is ideal for anyone who wants to build a domainâ€‘specific AI assistant with custom tone, behavior, and knowledge.

## ğŸš€ Features
- Fineâ€‘tuning using 4â€‘bit QLoRA
- Works on GPUs as small as 12â€“24 GB
- Instructionâ€‘style supervised fineâ€‘tuning (SFT)
- LoRA adapters applied to attention + MLP layers
- Evaluation against the original base model
- Easy to adapt for any dataset

## ğŸ§© Why Fineâ€‘Tune Qwen?
Qwen 2.5 models are strong generalâ€‘purpose LLMs, but they are not optimized for specialized domains.
**Fineâ€‘tuning allows you to:**
- Add domainâ€‘specific knowledge
- Customize tone and style
- Improve accuracy on niche tasks
- Reduce hallucinations
- Enforce custom safety rules
- Build predictable, consistent behavior
This makes Qwen 2.5 (1.5B) a great foundation for lightweight, specialized AI systems.

## ğŸ“¦ Dataset Format
**Your dataset must be in JSONL format:**
```bash
    {"instruction": "What is photosynthesis?", "output": "It is how plants make food using sunlight."}
    {"instruction": "Explain gravity simply.", "output": "Gravity pulls things toward the ground."}
```
**Required fields:**
- instruction â†’ user question
- output â†’ model answer

**Optional fields:**
- category
- metadata

## ğŸ› ï¸ Fineâ€‘Tuning Pipeline

**1. Load dataset**
Using datasets.load_dataset to read JSONL files.

**2. Load Qwen base model**
Loaded in 4â€‘bit quantized mode using BitsAndBytes.

**3. Prepare for QLoRA**
prepare_model_for_kbit_training() stabilizes training.

**4. Apply LoRA adapters**
Adapters are injected into:
- q_proj
- k_proj
- v_proj
- o_proj
- gate_proj
- up_proj
- down_proj
These layers control attention and MLP behavior.

**5. Train with SFTTrainer**
Supervised fineâ€‘tuning on your instruction â†’ output pairs.

**6. Evaluate**
Compare original vs fineâ€‘tuned model on test.jsonl

## ğŸ§° How to Fineâ€‘Tune on Your Own Dataset
**1. Prepare your dataset**
Create a JSONL file:
```bash
{"instruction": "...", "output": "..."}
```

**2. Update file paths**
In the training script:
```bash
train_data = load_dataset("json", data_files="train.jsonl")
```

**3. Adjust formatting function**
Example:
```bash
def format_example(e):
    return f"Instruction: {e['instruction']}\nAnswer: {e['output']}"
```

**4. Run training**
Use the provided QLoRA script.

**5. Evaluate**
Run the evaluation script to compare performance.

## ğŸ§‘â€ğŸ’» Example: Running Inference
```bash
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained("path/to/model")
tokenizer = AutoTokenizer.from_pretrained("path/to/model")

prompt = "Instruction: Explain gravity simply.\nAnswer:"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```
## ğŸ™Œ Acknowledgements
- Qwen team for the base model
- HuggingFace Transformers
- TRL (SFTTrainer)
- PEFT (LoRA)
- BitsAndBytes (4â€‘bit quantization)

## ğŸ‘¨â€ğŸ’» Author
Mohammed Abdul Bari

â­ Star this repo if it helps your social media game!






